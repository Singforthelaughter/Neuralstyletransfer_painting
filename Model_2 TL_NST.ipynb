{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Capstone TL_NST","provenance":[{"file_id":"1VFTnonB_cNuUDxQVGlMfDuZC-UBOd51t","timestamp":1583458250369},{"file_id":"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/style_transfer.ipynb","timestamp":1582031133389},{"file_id":"1ny1S8yiStnIk7WpVKw0NWcM2I2V4yEkV","timestamp":1570221686081},{"file_id":"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/style_transfer.ipynb","timestamp":1570217494261}],"private_outputs":true,"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"6msVLevwcRhm"},"source":["# Neural style transfer"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"eqxUicSPUOP6"},"source":["### Import and configure modules"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"NyftRTSMuwue","colab":{}},"source":["try:\n","  # %tensorflow_version only exists in Colab.\n","  %tensorflow_version 2.x\n","except Exception:\n","  pass\n","import tensorflow as tf"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"sc1OLbOWhPCO","colab":{}},"source":["%matplotlib inline\n","from __future__ import absolute_import, division, print_function, unicode_literals\n","\n","import matplotlib.pyplot as plt\n","\n","import numpy as np\n","\n","import keras\n","\n","from tensorflow.keras import optimizers\n","from tensorflow.keras.layers import Input\n","from tensorflow.keras.applications.vgg19 import VGG19\n","\n","from tensorflow.python.keras.preprocessing import image\n","from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n","\n","from keras import models\n","from tensorflow.python.keras.models import Model,load_model,Sequential\n","\n","from tensorflow.python.keras.layers import Dense, GlobalAveragePooling2D, Dropout,Flatten, Input, Conv2D, MaxPooling2D, Dropout, Flatten\n","from tensorflow.python.keras import backend as K\n","\n","from tensorflow.keras.callbacks import ModelCheckpoint, Callback\n","\n","from keras.preprocessing.image import load_img, img_to_array\n","\n","import IPython.display as display\n","import matplotlib as mpl\n","mpl.rcParams['figure.figsize'] = (12,12)\n","mpl.rcParams['axes.grid'] = False\n","\n","import PIL.Image\n","import time\n","import functools\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"GM6VEGrGLh62","colab":{}},"source":["def tensor_to_image(tensor):\n","  tensor = tensor*255\n","  tensor = np.array(tensor, dtype=np.uint8)\n","  if np.ndim(tensor)>3:\n","    assert tensor.shape[0] == 1\n","    tensor = tensor[0]\n","  return PIL.Image.fromarray(tensor)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JQ9TJ7yTZ-Az","colab_type":"code","colab":{}},"source":["def upload_files():\n","  from google.colab import files\n","  uploaded = files.upload()\n","  for k, v in uploaded.items():\n","    open(k, 'wb').write(v)\n","  return list(uploaded.keys())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"X-Pu5rF3aKlL","colab_type":"code","colab":{}},"source":["target_image_path = '/content/drive/My Drive/Colab Notebooks/img/shophouse.jpg'\n","style_reference_image_path = '/content/drive/My Drive/Colab Notebooks/img/wgz3.jpg'\n","width, height = load_img(target_image_path).size"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"xE4Yt8nArTeR"},"source":["## Visualize the input"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"klh6ObK2t_vH"},"source":["Define a function to load an image and limit its maximum dimension to 512 pixels."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"3TLljcwv5qZs","colab":{}},"source":["def load_image(path_to_img):\n","  max_dim = 512\n","  img = tf.io.read_file(path_to_img)\n","  img = tf.image.decode_image(img, channels=3)\n","  img = tf.image.convert_image_dtype(img, tf.float32)\n","\n","  shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n","  long_dim = max(shape)\n","  scale = max_dim / long_dim\n","\n","  new_shape = tf.cast(shape * scale, tf.int32)\n","\n","  img = tf.image.resize(img, new_shape)\n","  img = img[tf.newaxis, :]\n","  return img"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"2yAlRzJZrWM3"},"source":["Create a simple function to display an image:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"cBX-eNT8PAK_","colab":{}},"source":["def imshow(image, title=None):\n","  if len(image.shape) > 3:\n","    image = tf.squeeze(image, axis=0)\n","\n","  plt.imshow(image)\n","  if title:\n","    plt.title(title)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_UWQmeEaiKkP","colab":{}},"source":["content_image = load_image(target_image_path)\n","style_image = load_image(style_reference_image_path)\n","\n","plt.subplot(1, 2, 1)\n","imshow(content_image, 'Content Image')\n","\n","plt.subplot(1, 2, 2)\n","imshow(style_image, 'Style Image')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Yh_AV6220ebD","colab":{}},"source":["#vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n","model = load_model('/content/drive/My Drive/Colab Notebooks/TL_Trained_Model/vgg19_1_model.h5')\n","model.load_weights('/content/drive/My Drive/Colab Notebooks/TL_Trained_Model/vgg19_1_weight.h5')\n","\n","print()\n","for layer in model.layers:\n","  print(layer.name)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Wt-tASys0eJv"},"source":["Choose intermediate layers from the network to represent the style and content of the image:\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ArfX_6iA0WAX","colab":{}},"source":["# Content layer where will pull our feature maps\n","content_layers = ['block4_conv2'] \n","\n","# Style layer of interest\n","style_layers = ['block1_conv1',\n","                'block2_conv1',\n","                'block3_conv1', \n","                'block4_conv1', \n","                'block5_conv1',\n","                'conv2d_2']\n","\n","num_content_layers = len(content_layers)\n","num_style_layers = len(style_layers)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Jt3i3RRrJiOX"},"source":["## Build the model \n","\n","The networks in `tf.keras.applications` are designed so you can easily extract the intermediate layer values using the Keras functional API.\n","\n","To define a model using the functional API, specify the inputs and outputs:\n","\n","`model = Model(inputs, outputs)`\n","\n","This following function builds a VGG19 model that returns a list of intermediate layer outputs:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"nfec6MuMAbPx","colab":{}},"source":["def model_layers(layer_names):\n","  \"\"\" Creates a vgg model that returns a list of intermediate output values.\"\"\"\n","  # Load our model. Load pretrained VGG, trained on imagenet data\n","  model = load_model('/content/drive/My Drive/Colab Notebooks/TL_Trained_Model/vgg19_1_model.h5')\n","  model.load_weights('/content/drive/My Drive/Colab Notebooks/TL_Trained_Model/vgg19_1_weight.h5')\n","\n","  outputs = [model.get_layer(name).output for name in layer_names]\n","\n","  model = tf.keras.Model([model.input], outputs)\n","  return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"jbaIvZf5wWn_"},"source":["And to create the model:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"LkyvPpBHSfVi","colab":{}},"source":["style_extractor = model_layers(style_layers)\n","style_outputs = style_extractor(style_image*255)\n","\n","#Look at the statistics of each layer's output\n","for name, output in zip(style_layers, style_outputs):\n","  print(name)\n","  print(\"  shape: \", output.numpy().shape)\n","  print(\"  min: \", output.numpy().min())\n","  print(\"  max: \", output.numpy().max())\n","  print(\"  mean: \", output.numpy().mean())\n","  print()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"lGUfttK9F8d5"},"source":["## Calculate style\n","\n","The content of an image is represented by the values of the intermediate feature maps.\n","\n","It turns out, the style of an image can be described by the means and correlations across the different feature maps. Calculate a Gram matrix that includes this information by taking the outer product of the feature vector with itself at each location, and averaging that outer product over all locations. This Gram matrix can be calcualted for a particular layer as:\n","\n","$$G^l_{cd} = \\frac{\\sum_{ij} F^l_{ijc}(x)F^l_{ijd}(x)}{IJ}$$\n","\n","This can be implemented concisely using the `tf.linalg.einsum` function:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"HAy1iGPdoEpZ","colab":{}},"source":["def gram_matrix(input_tensor):\n","  result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)\n","  input_shape = tf.shape(input_tensor)\n","  num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)\n","  return result/(num_locations)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"pXIUX6czZABh"},"source":["## Extract style and content\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"1HGHvwlJ1nkn"},"source":["Build a model that returns the style and content tensors."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Sr6QALY-I1ja","colab":{}},"source":["class StyleContentModel(tf.keras.models.Model):\n","  def __init__(self, style_layers, content_layers):\n","    super(StyleContentModel, self).__init__()\n","    self.vgg =  model_layers(style_layers + content_layers)\n","    self.style_layers = style_layers\n","    self.content_layers = content_layers\n","    self.num_style_layers = len(style_layers)\n","    self.vgg.trainable = False\n","\n","  def call(self, inputs):\n","    \"Expects float input in [0,1]\"\n","    inputs = inputs*255.0\n","    preprocessed_input = tf.keras.applications.vgg19.preprocess_input(inputs)\n","    outputs = self.vgg(preprocessed_input)\n","    style_outputs, content_outputs = (outputs[:self.num_style_layers], \n","                                      outputs[self.num_style_layers:])\n","\n","    style_outputs = [gram_matrix(style_output)\n","                     for style_output in style_outputs]\n","\n","    content_dict = {content_name:value \n","                    for content_name, value \n","                    in zip(self.content_layers, content_outputs)}\n","\n","    style_dict = {style_name:value\n","                  for style_name, value\n","                  in zip(self.style_layers, style_outputs)}\n","    \n","    return {'content':content_dict, 'style':style_dict}"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Xuj1o33t1edl"},"source":["When called on an image, this model returns the gram matrix (style) of the `style_layers` and content of the `content_layers`:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"rkjO-DoNDU0A","colab":{}},"source":["extractor = StyleContentModel(style_layers, content_layers)\n","\n","results = extractor(tf.constant(content_image))\n","\n","style_results = results['style']\n","\n","print('Styles:')\n","for name, output in sorted(results['style'].items()):\n","  print(\"  \", name)\n","  print(\"    shape: \", output.numpy().shape)\n","  print(\"    min: \", output.numpy().min())\n","  print(\"    max: \", output.numpy().max())\n","  print(\"    mean: \", output.numpy().mean())\n","  print()\n","\n","print(\"Contents:\")\n","for name, output in sorted(results['content'].items()):\n","  print(\"  \", name)\n","  print(\"    shape: \", output.numpy().shape)\n","  print(\"    min: \", output.numpy().min())\n","  print(\"    max: \", output.numpy().max())\n","  print(\"    mean: \", output.numpy().mean())\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"y9r8Lyjb_m0u"},"source":["## Run gradient descent\n","\n","With this style and content extractor, you can now implement the style transfer algorithm. Do this by calculating the mean square error for your image's output relative to each target, then take the weighted sum of these losses.\n","\n","Set your style and content target values:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"PgkNOnGUFcKa","colab":{}},"source":["style_targets = extractor(style_image)['style']\n","content_targets = extractor(content_image)['content']"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"CNPrpl-e_w9A"},"source":["Define a `tf.Variable` to contain the image to optimize. To make this quick, initialize it with the content image (the `tf.Variable` must be the same shape as the content image):"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"M6L8ojmn_6rH"},"source":["Since this is a float image, define a function to keep the pixel values between 0 and 1:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"kdgpTJwL_vE2","colab":{}},"source":["def clip_0_1(image):\n","  return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"MBU5RFpcAo7W"},"source":["Create an optimizer. The paper recommends LBFGS, but `Adam` works okay, too:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"r4XZjqUk_5Eu","colab":{}},"source":["opt = tf.optimizers.Adam(learning_rate=0.02, beta_1=0.9, epsilon=1e-2)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"As-evbBiA2qT"},"source":["To optimize this, use a weighted combination of the two losses to get the total loss:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Dt4pxarvA4I4","colab":{}},"source":["style_weight=30\n","content_weight=6000\n","total_variation_weight=5"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"0ggx2Na8oROH","colab":{}},"source":["def style_content_loss(outputs):\n","    style_outputs = outputs['style']\n","    content_outputs = outputs['content']\n","    style_loss = tf.add_n([tf.reduce_mean((style_outputs[name]-style_targets[name])**2) \n","                           for name in style_outputs.keys()])\n","    style_loss *= style_weight / num_style_layers\n","\n","    content_loss = tf.add_n([tf.reduce_mean((content_outputs[name]-content_targets[name])**2) \n","                             for name in content_outputs.keys()])\n","    content_loss *= content_weight / num_content_layers\n","    loss = style_loss + content_loss\n","    return loss"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"GWVB3anJMY2v"},"source":["## Total variation loss\n","\n","One downside to this basic implementation is that it produces a lot of high frequency artifacts. Decrease these using an explicit regularization term on the high frequency components of the image. In style transfer, this is often called the *total variation loss*:"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"nTessd-DCdcC"},"source":["## Re-run the optimization\n","\n","Choose a weight for the `total_variation_loss`:"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"kG1-T4kJsoAv"},"source":["Now include it in the `train_step` function:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"BzmfcyyYUyWq","colab":{}},"source":["@tf.function()\n","def train_step(image):\n","  with tf.GradientTape() as tape:\n","    outputs = extractor(image)\n","    loss = style_content_loss(outputs)\n","    loss += total_variation_weight*tf.image.total_variation(image)\n","\n","  grad = tape.gradient(loss, image)\n","  opt.apply_gradients([(grad, image)])\n","  image.assign(clip_0_1(image))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"lcLWBQChsutQ"},"source":["Reinitialize the optimization variable:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"a-dPRr8BqexB","colab":{}},"source":["image = tf.Variable(content_image)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"BEflRstmtGBu"},"source":["And run the optimization:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"q3Cc3bLtoOWy","colab":{}},"source":["import time\n","start = time.time()\n","\n","epochs = 45\n","steps_per_epoch = 100\n","\n","step = 0\n","for n in range(epochs):\n","  for m in range(steps_per_epoch):\n","    step += 1\n","    train_step(image)\n","    print(\".\", end='')\n","  display.clear_output(wait=True)\n","  display.display(tensor_to_image(image))\n","  print(\"Train step: {}\".format(step))\n","\n","end = time.time()\n","print(\"Total time: {:.1f}\".format(end-start))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"KKox7K46tKxy"},"source":["Finally, save the result:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"SSH6OpyyQn7w","colab":{}},"source":["import cv2 as cv\n","image_final = tensor_to_image(image)\n","image_final = cv.fastNlMeansDenoisingColored(np.array(image_final),None,7,7,7,21)\n","image_final = cv.resize(image_final, (width, height), interpolation = cv.INTER_AREA)\n","plt.imshow(image_final)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-BGLjlQu0bWw","colab_type":"code","colab":{}},"source":["# save the image\n","plt.imsave('/content/drive/My Drive/Colab Notebooks/img/TL_model_img/NST06_shtest.png', image_final)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4CaaJO4xhl-v","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}